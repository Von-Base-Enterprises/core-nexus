"""
FastAPI REST API for Unified Memory Service

Provides HTTP endpoints for the Core Nexus Long Term Memory Module,
wrapping the UnifiedVectorStore with proper error handling and validation.
"""

import asyncio
import hashlib
import json
import logging
import os
import sys
import time
from contextlib import asynccontextmanager
from datetime import datetime
from typing import Any

from fastapi import BackgroundTasks, Depends, FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, Response, StreamingResponse
from prometheus_fastapi_instrumentator import Instrumentator

from .bulk_import_simple import (
    BulkImportRequest,
    BulkImportService,
    ImportProgress,
)
from .logging_config import get_logger, setup_logging
from .memory_export import (
    ExportRequest,
    MemoryExportService,
)
from .models import (
    HealthCheckResponse,
    MemoryRequest,
    MemoryResponse,
    MemoryStats,
    ProviderConfig,
    QueryRequest,
    QueryResponse,
)
from .providers import ChromaProvider, PgVectorProvider, PineconeProvider
from .unified_store import UnifiedVectorStore
from .observability import (
    initialize_observability,
    ObservabilityConfig,
    TraceRequestMiddleware,
    get_current_trace_id,
    trace_operation,
    record_metric
)

# Temporarily disable complex imports for stable deployment
# from .metrics import (
#     metrics_collector, get_metrics, record_request, time_request,
#     record_memory_operation, set_service_info
# )
# from .db_monitoring import get_database_health

# Setup logging with Papertrail support
setup_logging()
logger = get_logger("api")

# Global instances
unified_store: UnifiedVectorStore | None = None
usage_collector: Any = None  # Type: UsageCollector when implemented
memory_dashboard: Any = None  # Type: MemoryDashboard when implemented
bulk_import_service: BulkImportService | None = None
memory_export_service: MemoryExportService | None = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan management."""
    global unified_store, usage_collector, memory_dashboard, bulk_import_service, memory_export_service

    # Startup
    logger.info("Initializing Core Nexus Memory Service...")
    
    # Initialize OpenTelemetry observability
    try:
        observability_config = ObservabilityConfig()
        initialize_observability(app, observability_config)
        logger.info("OpenTelemetry observability initialized successfully")
    except Exception as e:
        logger.warning(f"Failed to initialize observability: {e}")
        # Continue without observability rather than failing startup

    # Initialize providers based on environment/config
    providers = []

    # Add pgvector if PostgreSQL is available
    # Use Render PostgreSQL internal hostname for better performance
    pgvector_host = os.getenv("PGVECTOR_HOST", "dpg-d12n0np5pdvs73ctmm40-a")

    # Validate required password is set (check both possible env var names)
    pgvector_password = os.getenv("PGPASSWORD") or os.getenv("PGVECTOR_PASSWORD")
    if not pgvector_password:
        logger.error("PGPASSWORD or PGVECTOR_PASSWORD environment variable is required but not set")
        raise ValueError("PGPASSWORD or PGVECTOR_PASSWORD must be set in environment variables")

    pgvector_config = ProviderConfig(
        name="pgvector",
        enabled=True,
        primary=False,  # Don't make primary unless it initializes successfully
        config={
            "host": pgvector_host,
            "port": int(os.getenv("PGVECTOR_PORT", "5432")),
            "database": os.getenv("PGVECTOR_DATABASE", "nexus_memory_db"),
            "user": os.getenv("PGVECTOR_USER", "nexus_memory_db_user"),
            "password": pgvector_password,
            "table_name": "vector_memories",
            "embedding_dim": 1536,
            "distance_metric": "cosine"
        }
    )
    try:
        # Use instrumented provider if observability is enabled
        if os.getenv("OTEL_TRACING_ENABLED", "true").lower() == "true":
            from .providers_instrumented import InstrumentedPgVectorProvider
            pgvector_provider = InstrumentedPgVectorProvider(pgvector_config)
            logger.info("Using instrumented PgVector provider")
        else:
            pgvector_provider = PgVectorProvider(pgvector_config)
        
        providers.append(pgvector_provider)
        pgvector_config.primary = True  # Make primary if successful
        logger.info("PgVector provider initialized as primary")
    except Exception as e:
        logger.warning(f"PgVector provider failed to initialize: {e}")
        pgvector_config.enabled = False

    # Add Pinecone if configured
    pinecone_config = ProviderConfig(
        name="pinecone",
        enabled=False,  # Disabled by default, enable with env var
        primary=False,
        config={
            "api_key": os.getenv("PINECONE_API_KEY", ""),
            "index_name": "core-nexus-memories",
            "embedding_dim": 1536
        }
    )
    try:
        pinecone_provider = PineconeProvider(pinecone_config)
        if pinecone_config.enabled:
            providers.append(pinecone_provider)
            logger.info("Pinecone provider initialized")
    except Exception as e:
        logger.warning(f"Pinecone provider failed to initialize: {e}")

    # Add ChromaDB (always available as local fallback)
    chroma_config = ProviderConfig(
        name="chromadb",
        enabled=True,
        primary=True,  # Make primary by default
        config={
            "collection_name": "core_nexus_memories",
            "persist_directory": "./memory_service_chroma"
        }
    )
    try:
        # Use instrumented provider if observability is enabled
        if os.getenv("OTEL_TRACING_ENABLED", "true").lower() == "true":
            from .providers_instrumented import InstrumentedChromaProvider
            chroma_provider = InstrumentedChromaProvider(chroma_config)
            logger.info("Using instrumented ChromaDB provider")
        else:
            chroma_provider = ChromaProvider(chroma_config)
        
        providers.append(chroma_provider)
        # If pgvector was successfully initialized, make it primary instead
        if any(p.name == "pgvector" and p.enabled for p in providers):
            chroma_config.primary = False
            logger.info("ChromaDB provider initialized as secondary (pgvector is primary)")
        else:
            logger.info("ChromaDB provider initialized as primary")
    except Exception as e:
        logger.error(f"ChromaDB provider failed to initialize: {e}")

    # Add Graph Provider for knowledge graph functionality
    # Feature flag controlled activation for safe rollout
    if os.getenv("GRAPH_ENABLED", "false").lower() == "true":
        logger.info("Graph provider enabled via GRAPH_ENABLED environment variable")

        # Check if pgvector is available to share the same database
        pgvector_provider = next((p for p in providers if p.name == 'pgvector' and p.enabled), None)
        if pgvector_provider:
            try:
                # Build connection string from pgvector config
                # This avoids timing issues with async pool initialization
                pg_config = pgvector_config.config
                connection_string = (
                    f"postgresql://{pg_config['user']}:{pg_config['password']}@"
                    f"{pg_config['host']}:{pg_config['port']}/{pg_config['database']}"
                )
                
                graph_config = ProviderConfig(
                    name="graph",
                    enabled=True,
                    primary=False,
                    config={
                        "connection_string": connection_string,  # Pass connection string instead of pool
                        "table_prefix": "graph"
                    }
                )

                # Import and initialize GraphProvider
                from .providers import GraphProvider
                graph_provider = GraphProvider(graph_config)
                providers.append(graph_provider)
                logger.info("âœ… Graph provider initialized successfully - Knowledge graph is ACTIVE!")

            except Exception as e:
                logger.error(f"Graph provider initialization failed: {e}")
                logger.info("Continuing without graph provider - system remains stable")
        else:
            logger.warning("Graph provider requires pgvector to be enabled")
    else:
        logger.info("Graph provider disabled (set GRAPH_ENABLED=true to activate)")

    if not providers:
        raise RuntimeError("No vector providers could be initialized")

    # Ensure we have at least one enabled primary provider
    enabled_providers = [p for p in providers if p.enabled]
    if not enabled_providers:
        raise RuntimeError("No enabled vector providers available")

    # If no primary provider is enabled, make the first enabled one primary
    has_enabled_primary = any(p.enabled and p.config.primary for p in providers)
    if not has_enabled_primary:
        enabled_providers[0].config.primary = True
        logger.warning(f"No enabled primary provider found, setting {enabled_providers[0].name} as primary")

    # Initialize OpenAI embedding model
    embedding_model = None
    try:
        from .embedding_models import create_embedding_model

        # Check if OpenAI API key is available
        openai_api_key = os.getenv("OPENAI_API_KEY")
        if openai_api_key and openai_api_key.strip():
            embedding_model = create_embedding_model(
                provider="openai",
                model="text-embedding-3-small",
                api_key=openai_api_key,
                max_retries=3,
                timeout=30.0
            )
            logger.info("Initialized OpenAI embedding model: text-embedding-3-small")
        else:
            embedding_model = create_embedding_model(provider="mock", dimension=1536)
            logger.warning("No OpenAI API key found, using mock embeddings")

    except Exception as e:
        logger.error(f"Failed to initialize embedding model: {e}")
        # Fallback to mock model
        from .embedding_models import MockEmbeddingModel
        embedding_model = MockEmbeddingModel(dimension=1536)
        logger.warning("Using mock embedding model as fallback")

    # Initialize unified store with ADM enabled and embedding model
    unified_store = UnifiedVectorStore(providers, embedding_model=embedding_model, adm_enabled=True)
    logger.info(f"Memory service started with {len(providers)} providers and {embedding_model.__class__.__name__}")

    # Initialize bulk import service (simplified version without Redis)
    global bulk_import_service, memory_export_service
    bulk_import_service = BulkImportService(unified_store)
    memory_export_service = MemoryExportService(unified_store)
    logger.info("Bulk import/export services initialized")

    # Initialize usage tracking - DISABLED FOR STABLE DEPLOYMENT
    # from .tracking import UsageCollector
    # usage_collector = UsageCollector(unified_store=unified_store)
    # logger.info("Usage tracking initialized")
    usage_collector = None

    # Initialize dashboard - DISABLED FOR STABLE DEPLOYMENT
    # from .dashboard import MemoryDashboard
    # memory_dashboard = MemoryDashboard(unified_store)
    # logger.info("Memory dashboard initialized")
    memory_dashboard = None

    # Set startup time for uptime tracking
    import time
    app.state.start_time = time.time()

    # Initialize service info metrics - DISABLED FOR STABLE DEPLOYMENT
    # set_service_info(
    #     version="0.1.0",
    #     config={
    #         "providers": [p.name for p in providers],
    #         "environment": os.getenv("ENVIRONMENT", "production")
    #     }
    # )

    yield

    # Shutdown
    logger.info("Shutting down Memory Service...")

    # Close provider connections
    for provider in providers:
        if hasattr(provider, 'close'):
            try:
                await provider.close()
            except Exception as e:
                logger.warning(f"Error closing provider {provider.name}: {e}")

    unified_store = None
    usage_collector = None
    memory_dashboard = None


def create_memory_app() -> FastAPI:
    """Create and configure the FastAPI application."""

    app = FastAPI(
        title="Core Nexus Memory Service",
        description="Unified Long Term Memory Module with multi-provider vector storage",
        version="0.1.0",
        lifespan=lifespan
    )

    # CORS middleware
    app.add_middleware(
        CORSMiddleware,
        allow_origins=os.getenv("ALLOWED_ORIGINS", "http://localhost:3000").split(","),
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    
    # Add OpenTelemetry request tracing middleware
    if os.getenv("OTEL_TRACING_ENABLED", "true").lower() == "true":
        app.middleware("http")(TraceRequestMiddleware(app))

    # FastAPI Prometheus Instrumentator for enhanced metrics
    instrumentator = Instrumentator(
        should_group_status_codes=True,
        should_ignore_untemplated=True,
        should_round_latency_decimals=True,
        excluded_handlers=["/metrics"],  # Don't track metrics endpoint itself
    )
    instrumentator.instrument(app)
    instrumentator.expose(app, endpoint="/metrics/fastapi")

    # Custom Prometheus metrics middleware
    @app.middleware("http")
    async def metrics_middleware(request: Request, call_next):
        start_time = time.time()

        # Process request
        response = await call_next(request)

        # Record metrics - DISABLED FOR STABLE DEPLOYMENT
        process_time = time.time() - start_time
        # record_request(
        #     method=request.method,
        #     endpoint=request.url.path,
        #     status_code=response.status_code
        # )

        # Add process time header
        response.headers["X-Process-Time"] = str(process_time)

        return response

    # Add usage tracking middleware - DISABLED FOR STABLE DEPLOYMENT
    # @app.on_event("startup")
    # async def add_usage_tracking():
    #     if usage_collector:
    #         from .tracking import UsageTrackingMiddleware
    #         app.add_middleware(UsageTrackingMiddleware, usage_collector=usage_collector)

    def get_store() -> UnifiedVectorStore:
        """Dependency to get the unified store instance."""
        if unified_store is None:
            raise HTTPException(
                status_code=503,
                detail="Memory service not initialized"
            )
        return unified_store

    @app.get("/health", response_model=HealthCheckResponse)
    async def health_check(store: UnifiedVectorStore = Depends(get_store)):
        """
        Check the health of all vector providers.

        Returns detailed status of each provider and overall service health.
        """
        try:
            health_data = await store.health_check()

            return HealthCheckResponse(
                status=health_data['status'],
                providers=health_data['providers'],
                total_memories=health_data['stats']['total_stores'],
                avg_query_time_ms=health_data['stats']['avg_query_time'],
                uptime_seconds=(time.time() - app.state.start_time) if hasattr(app.state, 'start_time') else 0
            )

        except Exception as e:
            logger.error(f"Health check failed: {e}")
            raise HTTPException(status_code=500, detail=str(e))

    # @app.get("/metrics") - DISABLED FOR STABLE DEPLOYMENT
    # async def metrics_endpoint():
    #     """
    #     Prometheus metrics endpoint.
    #
    #     Returns service metrics in Prometheus text format for monitoring and alerting.
    #     """
    #     try:
    #         # Collect current metrics
    #         if unified_store:
    #             await metrics_collector.collect_service_metrics(unified_store)
    #
    #         # Return Prometheus metrics
    #         metrics_data = get_metrics()
    #         return Response(
    #             content=metrics_data,
    #             media_type=CONTENT_TYPE_LATEST,
    #             headers={"Cache-Control": "no-cache"}
    #         )
    #     except Exception as e:
    #         logger.error(f"Metrics collection failed: {e}")
    #         raise HTTPException(status_code=500, detail="Metrics collection failed")

    # @app.get("/db/stats") - DISABLED FOR STABLE DEPLOYMENT
    # async def database_stats():
    #     """
    #     Database statistics and performance metrics.
    #
    #     Returns connection pool status, slow queries, and database health information.
    #     """
    #     try:
    #         health_data = await get_database_health()
    #         return JSONResponse(content=health_data)
    #     except Exception as e:
    #         logger.error(f"Database stats failed: {e}")
    #         raise HTTPException(status_code=500, detail="Database stats unavailable")

    @app.post("/memories", response_model=MemoryResponse)
    @trace_operation("api.store_memory")
    async def store_memory(
        request: MemoryRequest,
        background_tasks: BackgroundTasks,
        store: UnifiedVectorStore = Depends(get_store)
    ):
        """
        Store a new memory with automatic embedding generation.

        The memory will be stored across all enabled providers for resilience.
        """
        try:
            start_time = time.time()
            
            # Add tracing context
            trace_id = get_current_trace_id()
            if trace_id:
                logger.info(f"Storing memory with trace_id: {trace_id}")
            
            memory = await store.store_memory(request)

            # Log and record performance
            store_time = (time.time() - start_time) * 1000
            logger.info(f"Memory stored in {store_time:.1f}ms: {memory.id}")
            
            # Record metrics
            record_metric("memory_operations_total", 1, {"operation": "store", "status": "success"})
            record_metric("memory_operation_duration", store_time, {"operation": "store"})

            return memory

        except ValueError as e:
            record_metric("memory_operations_total", 1, {"operation": "store", "status": "error", "error_type": "validation"})
            raise HTTPException(status_code=400, detail=str(e))
        except Exception as e:
            record_metric("memory_operations_total", 1, {"operation": "store", "status": "error", "error_type": "internal"})
            logger.error(f"Failed to store memory: {e}")
            raise HTTPException(status_code=500, detail="Internal server error")

    @app.post("/memories/query", response_model=QueryResponse)
    @trace_operation("api.query_memories")
    async def query_memories(
        request: QueryRequest,
        store: UnifiedVectorStore = Depends(get_store)
    ):
        """
        Query memories using natural language.

        Returns semantically similar memories ranked by relevance and importance.
        Special handling: Empty queries return all memories (fixes 3-result bug).
        """
        try:
            start_time = time.time()
            
            # Add tracing attributes
            from opentelemetry import trace
            span = trace.get_current_span()
            if span:
                span.set_attribute("query.text", request.query[:100] if request.query else "")
                span.set_attribute("query.limit", request.limit)
                span.set_attribute("query.min_similarity", request.min_similarity)
                span.set_attribute("query.is_empty", not request.query or request.query.strip() == "")

            # Fix for empty query returning only 3 results
            if not request.query or request.query.strip() == "":
                logger.info(f"Empty query detected - returning all memories with limit {request.limit}")
                # For empty queries, set min_similarity to 0 to get all memories
                request.min_similarity = 0.0

            response = await store.query_memories(request)

            # Add request timing info
            total_time = (time.time() - start_time) * 1000
            logger.info(f"Query completed in {total_time:.1f}ms, found {response.total_found} memories, returned {len(response.memories)}")
            
            # Record metrics
            record_metric("memory_operations_total", 1, {"operation": "query", "status": "success"})
            record_metric("memory_operation_duration", total_time, {"operation": "query"})
            record_metric("vector_search_results", len(response.memories))
            
            # Add span attributes for results
            if span:
                span.set_attribute("results.total_found", response.total_found)
                span.set_attribute("results.returned", len(response.memories))
                span.set_attribute("results.query_time_ms", total_time)

            # Add trust metrics to build confidence
            response.trust_metrics = {
                "confidence_score": 1.0 if len(response.memories) > 3 else 0.3,
                "data_completeness": len(response.memories) / max(response.total_found, 1),
                "query_type": "empty_query" if not request.query.strip() else "semantic_search",
                "fix_applied": True,
                "expected_behavior": "Returns all memories up to limit for empty queries"
            }

            response.query_metadata = {
                "original_query": request.query,
                "limit_requested": request.limit,
                "actual_returned": len(response.memories),
                "api_version": "1.1.0-fixed"
            }

            return response

        except ValueError as e:
            record_metric("memory_operations_total", 1, {"operation": "query", "status": "error", "error_type": "validation"})
            raise HTTPException(status_code=400, detail=str(e))
        except Exception as e:
            record_metric("memory_operations_total", 1, {"operation": "query", "status": "error", "error_type": "internal"})
            logger.error(f"Query failed: {e}")
            raise HTTPException(status_code=500, detail="Internal server error")

    @app.get("/memories", response_model=QueryResponse)
    async def get_all_memories(
        limit: int = 100,
        offset: int = 0,
        store: UnifiedVectorStore = Depends(get_store)
    ):
        """
        Get all memories without search (returns most recent first).

        This endpoint addresses the issue where only 3 memories were returned.
        Now properly returns all memories with configurable limit.
        """
        try:
            # Use empty query to get all memories
            request = QueryRequest(
                query="",  # Empty query returns all memories
                limit=min(limit, 1000),  # Cap at 1000 for performance
                min_similarity=0.0  # Accept all memories
            )

            response = await store.query_memories(request)
            logger.info(f"GET /memories returned {len(response.memories)} of {response.total_found} total memories")

            # Add trust metrics
            response.trust_metrics = {
                "confidence_score": 1.0,
                "data_completeness": len(response.memories) / max(response.total_found, 1),
                "endpoint": "GET /memories",
                "fix_applied": True,
                "note": "This endpoint was added to fix the 3-result bug"
            }

            response.query_metadata = {
                "limit_requested": limit,
                "offset": offset,
                "actual_returned": len(response.memories),
                "total_available": response.total_found
            }

            return response

        except Exception as e:
            logger.error(f"Failed to get memories: {e}")
            raise HTTPException(status_code=500, detail="Internal server error")

    @app.get("/emergency/find-all-memories")
    async def emergency_find_all_memories(
        store: UnifiedVectorStore = Depends(get_store)
    ):
        """
        EMERGENCY: Find ALL memories in the database, regardless of embeddings.
        
        This endpoint bypasses all vector operations to ensure users can see their data.
        """
        try:
            pgvector = store.providers.get('pgvector')
            if not pgvector or not pgvector.enabled:
                raise HTTPException(status_code=503, detail="PgVector provider not available")
            
            from .search_fix import EmergencySearchFix
            emergency_search = EmergencySearchFix(pgvector.connection_pool)
            
            # Get diagnostic info
            diagnostics = await emergency_search.ensure_all_memories_visible()
            
            # Get all memories
            all_memories = await emergency_search.emergency_search_all(limit=10000)
            
            return {
                "status": "emergency_retrieval",
                "diagnostics": diagnostics,
                "total_memories_found": len(all_memories),
                "memories": [
                    {
                        "id": str(memory.id),
                        "content": memory.content[:200] + "..." if len(memory.content) > 200 else memory.content,
                        "created_at": memory.created_at,
                        "has_embedding": "unknown"
                    }
                    for memory in all_memories[:100]  # Show first 100
                ],
                "message": f"Found {len(all_memories)} total memories. Showing first 100.",
                "fix_instructions": "Use /memories/search/text?q=your_query for text-based search"
            }
            
        except Exception as e:
            logger.error(f"Emergency retrieval failed: {e}")
            raise HTTPException(status_code=500, detail=f"Emergency retrieval failed: {str(e)}")

    @app.get("/memories/search/text")
    async def text_search_memories(
        q: str,
        limit: int = 100,
        store: UnifiedVectorStore = Depends(get_store)
    ):
        """
        Text-based search fallback when vector search fails.
        
        Uses PostgreSQL full-text search and fuzzy matching.
        """
        try:
            pgvector = store.providers.get('pgvector')
            if not pgvector or not pgvector.enabled:
                raise HTTPException(status_code=503, detail="PgVector provider not available")
            
            from .search_fix import EmergencySearchFix
            emergency_search = EmergencySearchFix(pgvector.connection_pool)
            
            # Try text search first
            memories = await emergency_search.text_search(q, limit=limit)
            
            # If no results, try fuzzy search
            if not memories:
                memories = await emergency_search.fuzzy_search(q, limit=limit)
            
            return {
                "query": q,
                "results_found": len(memories),
                "search_type": "text_based",
                "memories": [
                    {
                        "id": str(memory.id),
                        "content": memory.content,
                        "relevance_score": memory.similarity_score,
                        "created_at": memory.created_at
                    }
                    for memory in memories
                ]
            }
            
        except Exception as e:
            logger.error(f"Text search failed: {e}")
            raise HTTPException(status_code=500, detail=f"Text search failed: {str(e)}")

    @app.get("/memories/stats", response_model=MemoryStats)
    async def get_memory_stats(store: UnifiedVectorStore = Depends(get_store)):
        """
        Get comprehensive memory service statistics.

        Includes counts, performance metrics, and provider-specific details.
        """
        try:
            stats = store.stats
            health_data = await store.health_check()

            # Calculate provider-specific stats
            memories_by_provider = {}
            for provider_name, provider_health in health_data['providers'].items():
                if 'details' in provider_health and 'total_vectors' in provider_health['details']:
                    memories_by_provider[provider_name] = provider_health['details']['total_vectors']
                else:
                    memories_by_provider[provider_name] = 0

            # Get actual total from provider stats
            actual_total = sum(memories_by_provider.values())

            return MemoryStats(
                total_memories=actual_total if actual_total > 0 else stats['total_stores'],
                memories_by_provider=memories_by_provider,
                avg_importance_score=0.5,  # TODO: Calculate from actual data
                queries_last_hour=stats['total_queries'],  # TODO: Implement time-based tracking
                avg_query_time_ms=stats['avg_query_time']
            )

        except Exception as e:
            logger.error(f"Failed to get stats: {e}")
            raise HTTPException(status_code=500, detail="Internal server error")

    @app.get("/debug/env")
    async def debug_environment():
        """Debug endpoint to check environment variables."""
        import os

        # Check various environment variables
        env_status = {
            "openai": {
                "OPENAI_API_KEY": {
                    "present": bool(os.getenv("OPENAI_API_KEY")),
                    "length": len(os.getenv("OPENAI_API_KEY", "")) if os.getenv("OPENAI_API_KEY") else 0,
                    "starts_with": os.getenv("OPENAI_API_KEY", "")[:7] if os.getenv("OPENAI_API_KEY") else "NOT_SET"
                }
            },
            "postgresql": {
                "PGVECTOR_HOST": os.getenv("PGVECTOR_HOST", "NOT_SET"),
                "PGVECTOR_PORT": os.getenv("PGVECTOR_PORT", "NOT_SET"),
                "PGVECTOR_DATABASE": os.getenv("PGVECTOR_DATABASE", "NOT_SET"),
                "PGVECTOR_USER": os.getenv("PGVECTOR_USER", "NOT_SET"),
                "PGVECTOR_PASSWORD": {
                    "present": bool(os.getenv("PGVECTOR_PASSWORD")),
                    "length": len(os.getenv("PGVECTOR_PASSWORD", "")) if os.getenv("PGVECTOR_PASSWORD") else 0
                }
            },
            "render": {
                "RENDER": os.getenv("RENDER", "NOT_SET"),
                "RENDER_SERVICE_NAME": os.getenv("RENDER_SERVICE_NAME", "NOT_SET"),
                "RENDER_SERVICE_TYPE": os.getenv("RENDER_SERVICE_TYPE", "NOT_SET"),
                "RENDER_GIT_COMMIT": os.getenv("RENDER_GIT_COMMIT", "NOT_SET")[:8] if os.getenv("RENDER_GIT_COMMIT") else "NOT_SET"
            },
            "service": {
                "SERVICE_NAME": os.getenv("SERVICE_NAME", "NOT_SET"),
                "ENVIRONMENT": os.getenv("ENVIRONMENT", "NOT_SET"),
                "LOG_LEVEL": os.getenv("LOG_LEVEL", "NOT_SET"),
                "CORS_ORIGINS": os.getenv("CORS_ORIGINS", "NOT_SET")
            },
            "embedding_model": unified_store.embedding_model.__class__.__name__ if unified_store and unified_store.embedding_model else "None",
            "primary_provider": unified_store.primary_provider.name if unified_store and unified_store.primary_provider else "None"
        }

        return env_status

    @app.get("/debug/logs")
    async def get_recent_logs(lines: int = 100):
        """
        Get recent application logs for debugging.

        Returns last N lines of logs with timestamps and levels.
        """
        try:
            from collections import deque
            from datetime import datetime

            # Create in-memory log buffer if not exists
            if not hasattr(app.state, 'log_buffer'):
                app.state.log_buffer = deque(maxlen=1000)

                # Set up log capture handler
                import logging

                class BufferHandler(logging.Handler):
                    def emit(self, record):
                        try:
                            log_entry = {
                                'timestamp': datetime.fromtimestamp(record.created).isoformat(),
                                'level': record.levelname,
                                'logger': record.name,
                                'message': self.format(record),
                                'module': record.module,
                                'function': record.funcName,
                                'line': record.lineno
                            }
                            app.state.log_buffer.append(log_entry)
                        except Exception:
                            pass

                # Add handler to root logger
                buffer_handler = BufferHandler()
                buffer_handler.setLevel(logging.DEBUG)
                buffer_handler.setFormatter(logging.Formatter(
                    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
                ))
                logging.getLogger().addHandler(buffer_handler)

                # Also add to our logger
                logger.addHandler(buffer_handler)

                # Log that we started capturing
                logger.info("Log capture initialized for debug endpoint")

            # Get requested number of recent logs
            recent_logs = list(app.state.log_buffer)[-lines:]

            # Add some system info
            system_info = {
                'python_version': sys.version,
                'service_uptime_seconds': time.time() - app.state.start_time if hasattr(app.state, 'start_time') else 0,
                'log_buffer_size': len(app.state.log_buffer),
                'providers_status': {
                    name: {
                        'enabled': provider.enabled,
                        'primary': provider == unified_store.primary_provider
                    }
                    for name, provider in unified_store.providers.items()
                } if unified_store else {},
                'embedding_model': unified_store.embedding_model.__class__.__name__ if unified_store and unified_store.embedding_model else None
            }

            return {
                'logs': recent_logs,
                'total_logs_captured': len(app.state.log_buffer),
                'logs_returned': len(recent_logs),
                'system_info': system_info
            }

        except Exception as e:
            logger.error(f"Failed to get logs: {e}")
            return {
                'error': str(e),
                'logs': [],
                'message': 'Log capture may not be initialized yet'
            }

    @app.get("/debug/startup-logs")
    async def get_startup_logs():
        """
        Get startup and initialization logs.

        Shows what happened during service initialization.
        """
        # Create a summary of startup state
        startup_info = {
            'service_status': 'running',
            'uptime_seconds': time.time() - app.state.start_time if hasattr(app.state, 'start_time') else 0,
            'providers': {},
            'embedding_model': None,
            'initialization_errors': []
        }

        # Check providers
        if unified_store:
            for name, provider in unified_store.providers.items():
                startup_info['providers'][name] = {
                    'enabled': provider.enabled,
                    'primary': provider == unified_store.primary_provider,
                    'status': 'active' if provider.enabled else 'disabled'
                }

            # Check embedding model
            if unified_store.embedding_model:
                startup_info['embedding_model'] = {
                    'type': unified_store.embedding_model.__class__.__name__,
                    'dimension': unified_store.embedding_model.dimension
                }

                # Check if it's OpenAI and why it might have failed
                if startup_info['embedding_model']['type'] == 'MockEmbeddingModel':
                    import os
                    api_key = os.getenv("OPENAI_API_KEY", "")
                    if not api_key:
                        startup_info['initialization_errors'].append(
                            "OPENAI_API_KEY environment variable not found"
                        )
                    elif api_key == "mock_key_for_demo":
                        startup_info['initialization_errors'].append(
                            "OPENAI_API_KEY is set to mock value from render.yaml"
                        )

        # Check for common issues
        if 'pgvector' in startup_info['providers'] and not startup_info['providers']['pgvector']['enabled']:
            startup_info['initialization_errors'].append(
                "PgVector provider failed to initialize - PostgreSQL connection refused"
            )

        return startup_info

    @app.get("/logs/stream")
    async def stream_logs(format: str = "json"):
        """
        Stream logs in real-time via Server-Sent Events (SSE).

        Formats:
        - json: JSON formatted logs
        - syslog: Syslog format (RFC3164) compatible with Papertrail
        - plain: Plain text logs

        Usage:
        - curl https://service.com/logs/stream
        - curl https://service.com/logs/stream?format=syslog
        """
        import queue
        from datetime import datetime

        # Create queue for log streaming
        log_queue = queue.Queue(maxsize=100)

        # Custom handler to capture logs
        class StreamHandler(logging.Handler):
            def emit(self, record):
                try:
                    if format == "syslog":
                        # Syslog format: <priority>timestamp hostname app[pid]: message
                        priority = self._get_syslog_priority(record.levelno)
                        timestamp = datetime.fromtimestamp(record.created).strftime('%b %d %H:%M:%S')
                        hostname = os.getenv('RENDER_SERVICE_NAME', 'core-nexus-memory')
                        pid = os.getpid()
                        message = self.format(record)
                        log_line = f"<{priority}>{timestamp} {hostname} {record.name}[{pid}]: {message}\n"
                    elif format == "plain":
                        log_line = f"{self.format(record)}\n"
                    else:  # json
                        log_data = {
                            'timestamp': datetime.fromtimestamp(record.created).isoformat(),
                            'level': record.levelname,
                            'logger': record.name,
                            'message': self.format(record),
                            'module': record.module,
                            'function': record.funcName,
                            'line': record.lineno
                        }
                        log_line = f"data: {json.dumps(log_data)}\n\n"

                    # Non-blocking put
                    log_queue.put_nowait(log_line)
                except queue.Full:
                    pass  # Drop log if queue is full
                except Exception:
                    pass

            def _get_syslog_priority(self, levelno):
                """Convert Python log level to syslog priority."""
                # Facility = 16 (local0), Severity based on level
                facility = 16
                if levelno >= 50:  # CRITICAL
                    severity = 2
                elif levelno >= 40:  # ERROR
                    severity = 3
                elif levelno >= 30:  # WARNING
                    severity = 4
                elif levelno >= 20:  # INFO
                    severity = 6
                else:  # DEBUG
                    severity = 7
                return facility * 8 + severity

        # Add handler to capture logs
        stream_handler = StreamHandler()
        stream_handler.setLevel(logging.DEBUG)
        stream_handler.setFormatter(logging.Formatter('%(message)s'))

        # Add to root logger and our logger
        root_logger = logging.getLogger()
        root_logger.addHandler(stream_handler)
        logger.addHandler(stream_handler)

        async def generate():
            """Generate log stream."""
            try:
                # Send initial connection message
                if format == "json":
                    yield f"data: {json.dumps({'connected': True, 'format': format})}\n\n"
                elif format == "syslog":
                    yield f"<134>{datetime.now().strftime('%b %d %H:%M:%S')} {os.getenv('RENDER_SERVICE_NAME', 'core-nexus')} logger[{os.getpid()}]: Log streaming connected\n"
                else:
                    yield "Log streaming connected\n"

                # Stream logs
                while True:
                    try:
                        # Get log with timeout
                        log_line = await asyncio.get_event_loop().run_in_executor(
                            None, log_queue.get, True, 1.0
                        )
                        yield log_line

                        # Send keepalive every 30 seconds
                        if format == "json" and asyncio.get_event_loop().time() % 30 < 1:
                            yield f"data: {json.dumps({'keepalive': True})}\n\n"

                    except queue.Empty:
                        # Send keepalive
                        if format == "json":
                            yield "\n"  # SSE keepalive
                        await asyncio.sleep(0.1)

            finally:
                # Cleanup
                root_logger.removeHandler(stream_handler)
                logger.removeHandler(stream_handler)

        # Return appropriate response type
        if format == "json":
            return StreamingResponse(
                generate(),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "X-Accel-Buffering": "no"  # Disable Nginx buffering
                }
            )
        else:
            return StreamingResponse(
                generate(),
                media_type="text/plain",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "X-Accel-Buffering": "no"
                }
            )

    @app.get("/logs/syslog")
    async def syslog_endpoint(request: Request):
        """
        Syslog endpoint information for external log aggregation.

        Returns connection details for setting up syslog forwarding.
        """
        return {
            "message": "To stream logs to an external syslog server like Papertrail:",
            "options": {
                "1_streaming_endpoint": {
                    "description": "Use our streaming endpoint",
                    "url": f"{request.url.scheme}://{request.url.netloc}/logs/stream?format=syslog",
                    "method": "GET",
                    "format": "RFC3164 syslog format"
                },
                "2_environment_config": {
                    "description": "Configure via environment variables",
                    "PAPERTRAIL_HOST": "logs.papertrailapp.com",
                    "PAPERTRAIL_PORT": "34949",
                    "note": "Logs will be automatically forwarded if these are set"
                },
                "3_render_integration": {
                    "description": "Add log drain in Render.com dashboard",
                    "format": "syslog+tls://logs.papertrailapp.com:34949",
                    "location": "Settings -> Log Streams"
                }
            },
            "current_config": {
                "papertrail_configured": bool(os.getenv("PAPERTRAIL_HOST")),
                "service_name": os.getenv("RENDER_SERVICE_NAME", "core-nexus-memory")
            }
        }

    @app.get("/providers")
    async def list_providers(store: UnifiedVectorStore = Depends(get_store)):
        """
        List all configured vector providers and their status.

        Useful for monitoring and debugging provider configurations.
        """
        try:
            provider_info = []

            for name, provider in store.providers.items():
                provider_stats = await provider.get_stats()
                provider_info.append({
                    'name': name,
                    'enabled': provider.enabled,
                    'primary': provider == store.primary_provider,
                    'config': {
                        'retry_count': provider.config.retry_count,
                        'timeout_seconds': provider.config.timeout_seconds
                    },
                    'stats': provider_stats
                })

            # Add embedding model info
            embedding_info = {
                'model_type': store.embedding_model.__class__.__name__ if store.embedding_model else None,
                'dimension': store.embedding_model.dimension if store.embedding_model else None
            }

            # Add health check for embedding model if it's OpenAI
            if hasattr(store.embedding_model, 'health_check'):
                try:
                    embedding_health = await store.embedding_model.health_check()
                    embedding_info['health'] = embedding_health
                except Exception as e:
                    embedding_info['health'] = {'status': 'error', 'error': str(e)}

            return {
                'providers': provider_info,
                'primary_provider': store.primary_provider.name,
                'total_providers': len(store.providers),
                'embedding_model': embedding_info
            }

        except Exception as e:
            logger.error(f"Failed to list providers: {e}")
            raise HTTPException(status_code=500, detail="Internal server error")

    @app.post("/embeddings/test")
    async def test_embedding(
        text: str,
        store: UnifiedVectorStore = Depends(get_store)
    ):
        """
        Test the embedding functionality with provided text.

        Useful for verifying OpenAI integration and debugging embedding issues.
        """
        try:
            if not store.embedding_model:
                raise HTTPException(status_code=503, detail="No embedding model configured")

            start_time = time.time()
            embedding = await store.embedding_model.embed_text(text)
            duration = (time.time() - start_time) * 1000

            return {
                'text': text,
                'embedding_dimension': len(embedding),
                'embedding_sample': embedding[:5],  # First 5 values for verification
                'model_type': store.embedding_model.__class__.__name__,
                'generation_time_ms': round(duration, 2),
                'success': True
            }

        except Exception as e:
            logger.error(f"Embedding test failed: {e}")
            raise HTTPException(status_code=500, detail=f"Embedding test failed: {str(e)}")

    @app.post("/memories/batch", response_model=list[MemoryResponse])
    async def store_memories_batch(
        requests: list[MemoryRequest],
        store: UnifiedVectorStore = Depends(get_store)
    ):
        """
        Store multiple memories in batch for better performance.

        Processes memories concurrently while maintaining data integrity.
        """
        try:
            if len(requests) > 100:
                raise HTTPException(
                    status_code=400,
                    detail="Batch size limited to 100 memories"
                )

            start_time = time.time()

            # Process batch concurrently
            tasks = [store.store_memory(req) for req in requests]
            memories = await asyncio.gather(*tasks, return_exceptions=True)

            # Handle any failures
            successful_memories = []
            for i, memory in enumerate(memories):
                if isinstance(memory, Exception):
                    logger.error(f"Failed to store memory {i}: {memory}")
                else:
                    successful_memories.append(memory)

            batch_time = (time.time() - start_time) * 1000
            logger.info(f"Batch stored {len(successful_memories)}/{len(requests)} memories in {batch_time:.1f}ms")

            return successful_memories

        except ValueError as e:
            raise HTTPException(status_code=400, detail=str(e))
        except Exception as e:
            logger.error(f"Batch store failed: {e}")
            raise HTTPException(status_code=500, detail="Internal server error")

    # =====================================================
    # BULK IMPORT API - Enterprise Features
    # =====================================================

    @app.post("/api/v1/memories/import")
    async def import_memories_bulk(
        request: BulkImportRequest,
        background_tasks: BackgroundTasks
    ):
        """
        Import memories in bulk from CSV, JSON, or JSONL format.

        This endpoint starts an asynchronous import job and returns immediately
        with a job ID for tracking progress.

        Supports:
        - CSV with content column and optional metadata columns
        - JSON array or object with memories array
        - JSONL (newline-delimited JSON) for streaming large datasets

        Features:
        - Automatic deduplication
        - Validation and error handling
        - Progress tracking
        - Batch processing for performance
        """
        if not bulk_import_service:
            raise HTTPException(
                status_code=503,
                detail="Bulk import service not available"
            )

        try:
            result = await bulk_import_service.import_memories(request, background_tasks)
            return result
        except ValueError as e:
            raise HTTPException(status_code=400, detail=str(e))
        except Exception as e:
            logger.error(f"Bulk import failed: {e}")
            raise HTTPException(status_code=500, detail="Import initialization failed")

    @app.get("/api/v1/memories/import/{import_id}/status", response_model=ImportProgress)
    async def get_import_status(import_id: str):
        """
        Get the status of a bulk import job.

        Returns detailed progress information including:
        - Current status (pending, processing, completed, failed)
        - Records processed/successful/failed
        - Estimated completion time
        - Any errors encountered
        """
        if not bulk_import_service:
            raise HTTPException(
                status_code=503,
                detail="Bulk import service not available"
            )

        try:
            progress = await bulk_import_service.get_import_status(import_id)
            return progress
        except ValueError:
            raise HTTPException(status_code=400, detail="Invalid import ID format")
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"Failed to get import status: {e}")
            raise HTTPException(status_code=500, detail="Failed to retrieve import status")

    # =====================================================
    # MEMORY EXPORT API - Data Portability
    # =====================================================

    @app.post("/api/v1/memories/export")
    async def export_memories(request: ExportRequest):
        """
        Export memories in various formats with filtering.

        Supports:
        - JSON: Complete data with metadata
        - CSV: Spreadsheet-compatible format
        - PDF: Formatted document (coming soon)

        Features:
        - Date range filtering
        - Importance score filtering
        - Tag-based filtering
        - Optional embedding inclusion
        - GDPR-compliant export option
        """
        if not memory_export_service:
            raise HTTPException(
                status_code=503,
                detail="Export service not available"
            )

        try:
            return await memory_export_service.export_memories(request)
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"Export failed: {e}")
            raise HTTPException(status_code=500, detail="Export failed")

    @app.get("/api/v1/memories/export/gdpr/{user_id}")
    async def export_gdpr_package(user_id: str):
        """
        Export GDPR-compliant data package for a specific user.

        Creates a comprehensive data export including:
        - All user memories
        - Complete metadata
        - Data sources and processing information
        - Export metadata and timestamps

        Compliant with GDPR Article 20 (Right to Data Portability)
        """
        if not memory_export_service:
            raise HTTPException(
                status_code=503,
                detail="Export service not available"
            )

        try:
            return await memory_export_service.create_gdpr_package(user_id)
        except Exception as e:
            logger.error(f"GDPR export failed: {e}")
            raise HTTPException(status_code=500, detail="GDPR export failed")

    @app.delete("/memories/cache")
    async def clear_query_cache(store: UnifiedVectorStore = Depends(get_store)):
        """
        Clear the query result cache.

        Use this when you need fresh results or after significant data updates.
        """
        try:
            cache_size = len(store.query_cache)
            store.query_cache.clear()

            return {
                'message': f'Cleared {cache_size} cached queries',
                'cache_size_before': cache_size,
                'cache_size_after': 0
            }

        except Exception as e:
            logger.error(f"Failed to clear cache: {e}")
            raise HTTPException(status_code=500, detail="Internal server error")

    # =============================================================================
    # DASHBOARD AND ANALYTICS ENDPOINTS
    # =============================================================================

    @app.get("/dashboard/metrics")
    async def get_dashboard_metrics():
        """
        Get comprehensive dashboard metrics.

        Provides real-time insights into memory service performance and quality.
        """
        try:
            if not memory_dashboard:
                raise HTTPException(status_code=503, detail="Dashboard not initialized")

            metrics = await memory_dashboard.get_comprehensive_metrics()
            return metrics.to_dict()

        except Exception as e:
            logger.error(f"Dashboard metrics failed: {e}")
            raise HTTPException(status_code=500, detail="Failed to get dashboard metrics")

    @app.get("/dashboard/quality-trends")
    async def get_quality_trends(days: int = 7):
        """
        Get memory quality trends over time.

        Shows how memory quality has evolved over the specified period.
        """
        try:
            if not memory_dashboard:
                raise HTTPException(status_code=503, detail="Dashboard not initialized")

            trends = await memory_dashboard.get_quality_trends(days=days)
            return {"trends": trends, "period_days": days}

        except Exception as e:
            logger.error(f"Quality trends failed: {e}")
            raise HTTPException(status_code=500, detail="Failed to get quality trends")

    @app.get("/dashboard/provider-performance")
    async def get_provider_performance():
        """
        Get detailed performance metrics for each vector provider.

        Includes health, performance, and feature comparison.
        """
        try:
            if not memory_dashboard:
                raise HTTPException(status_code=503, detail="Dashboard not initialized")

            performance = await memory_dashboard.get_provider_performance()
            return {"providers": performance}

        except Exception as e:
            logger.error(f"Provider performance failed: {e}")
            raise HTTPException(status_code=500, detail="Failed to get provider performance")

    @app.get("/dashboard/insights")
    async def get_memory_insights(limit: int = 50):
        """
        Get insights about memory patterns and usage.

        Identifies trends, patterns, and optimization opportunities.
        """
        try:
            if not memory_dashboard:
                raise HTTPException(status_code=503, detail="Dashboard not initialized")

            insights = await memory_dashboard.get_memory_insights(limit=limit)
            return insights

        except Exception as e:
            logger.error(f"Memory insights failed: {e}")
            raise HTTPException(status_code=500, detail="Failed to get memory insights")

    # =============================================================================
    # ADM SCORING AND INTELLIGENCE ENDPOINTS
    # =============================================================================

    @app.get("/adm/performance")
    async def get_adm_performance(store: UnifiedVectorStore = Depends(get_store)):
        """
        Get ADM scoring engine performance metrics.

        Shows how well the automated decision making is performing.
        """
        try:
            if not memory_dashboard:
                raise HTTPException(status_code=503, detail="Dashboard not initialized")

            adm_perf = await memory_dashboard.get_adm_performance()
            return adm_perf

        except Exception as e:
            logger.error(f"ADM performance failed: {e}")
            raise HTTPException(status_code=500, detail="Failed to get ADM performance")

    @app.post("/adm/analyze")
    async def analyze_content_adm(
        content: str,
        metadata: dict[str, Any] | None = None,
        store: UnifiedVectorStore = Depends(get_store)
    ):
        """
        Analyze content using ADM scoring without storing.

        Provides detailed breakdown of data quality, relevance, and intelligence.
        """
        try:
            if not store.adm_enabled or not store.adm_engine:
                raise HTTPException(status_code=400, detail="ADM scoring not enabled")

            analysis = await store.adm_engine.calculate_adm_score(
                content=content,
                metadata=metadata or {}
            )

            return {
                "analysis": analysis,
                "recommendations": {
                    "store_recommended": analysis["adm_score"] > 0.5,
                    "importance_level": "high" if analysis["adm_score"] > 0.7 else "medium" if analysis["adm_score"] > 0.4 else "low",
                    "quality_notes": {
                        "data_quality": "excellent" if analysis["data_quality"] > 0.8 else "good" if analysis["data_quality"] > 0.6 else "needs_improvement",
                        "data_relevance": "high" if analysis["data_relevance"] > 0.8 else "medium" if analysis["data_relevance"] > 0.6 else "low",
                        "data_intelligence": "high" if analysis["data_intelligence"] > 0.8 else "medium" if analysis["data_intelligence"] > 0.6 else "low"
                    }
                }
            }

        except Exception as e:
            logger.error(f"ADM analysis failed: {e}")
            raise HTTPException(status_code=500, detail="Failed to analyze content")

    @app.post("/adm/suggest-evolution/{memory_id}")
    async def suggest_memory_evolution(
        memory_id: str,
        store: UnifiedVectorStore = Depends(get_store)
    ):
        """
        Suggest evolution strategy for a specific memory.

        Uses Darwin-GÃ¶del principles to recommend memory lifecycle actions.
        """
        try:
            if not store.adm_enabled or not store.adm_engine:
                raise HTTPException(status_code=400, detail="ADM scoring not enabled")

            # TODO: Get memory by ID from providers
            # For now, return mock suggestion

            strategy, confidence = await store.adm_engine.suggest_evolution_strategy(
                memory=None  # TODO: Implement memory retrieval by ID
            )

            return {
                "memory_id": memory_id,
                "suggested_strategy": strategy.value,
                "confidence_score": confidence,
                "reasoning": f"Based on ADM analysis, {strategy.value} is recommended with {confidence:.1%} confidence",
                "next_steps": {
                    "reinforcement": "Increase importance score and access frequency",
                    "diversification": "Explore related topics and expand context",
                    "consolidation": "Merge with similar high-value memories",
                    "pruning": "Consider archival or removal due to low value"
                }.get(strategy.value, "Monitor and maintain current state")
            }

        except Exception as e:
            logger.error(f"Evolution suggestion failed: {e}")
            raise HTTPException(status_code=500, detail="Failed to suggest evolution")

    # =============================================================================
    # USAGE TRACKING AND ANALYTICS ENDPOINTS
    # =============================================================================

    @app.get("/analytics/usage")
    async def get_usage_analytics():
        """
        Get comprehensive usage analytics and patterns.

        Provides insights into system performance and user behavior.
        """
        try:
            if not usage_collector:
                raise HTTPException(status_code=503, detail="Usage tracking not initialized")

            performance_metrics = usage_collector.get_performance_metrics()
            usage_patterns = usage_collector.get_usage_patterns()

            return {
                "performance": {
                    "total_requests": performance_metrics.total_requests,
                    "avg_response_time_ms": performance_metrics.avg_response_time_ms,
                    "p95_response_time_ms": performance_metrics.p95_response_time_ms,
                    "p99_response_time_ms": performance_metrics.p99_response_time_ms,
                    "error_rate": performance_metrics.error_rate,
                    "requests_per_minute": performance_metrics.requests_per_minute,
                    "memory_operations_per_minute": performance_metrics.memory_operations_per_minute,
                    "unique_users_last_hour": performance_metrics.unique_users_last_hour
                },
                "patterns": usage_patterns
            }

        except Exception as e:
            logger.error(f"Usage analytics failed: {e}")
            raise HTTPException(status_code=500, detail="Failed to get usage analytics")

    @app.get("/analytics/export")
    async def export_analytics(format: str = "json", limit: int | None = None):
        """
        Export usage events and analytics data.

        Supports JSON and CSV formats for external analysis.
        """
        try:
            if not usage_collector:
                raise HTTPException(status_code=503, detail="Usage tracking not initialized")

            if not memory_dashboard:
                raise HTTPException(status_code=503, detail="Dashboard not initialized")

            # Export comprehensive data
            if format.lower() == "comprehensive":
                export_data = await memory_dashboard.export_metrics(format="json")
                return JSONResponse(
                    content={"data": export_data},
                    headers={"Content-Disposition": "attachment; filename=memory_service_export.json"}
                )
            else:
                # Export usage events
                events_data = usage_collector.export_events(format=format, limit=limit)

                if format.lower() == "json":
                    return JSONResponse(
                        content={"events": events_data},
                        headers={"Content-Disposition": "attachment; filename=usage_events.json"}
                    )
                else:
                    return Response(
                        content=events_data,
                        media_type="text/csv",
                        headers={"Content-Disposition": "attachment; filename=usage_events.csv"}
                    )

        except Exception as e:
            logger.error(f"Analytics export failed: {e}")
            raise HTTPException(status_code=500, detail="Failed to export analytics")

    @app.post("/analytics/feedback")
    async def record_feedback(
        memory_id: str,
        useful: bool,
        feedback_type: str = "general",
        notes: str | None = None,
        store: UnifiedVectorStore = Depends(get_store)
    ):
        """
        Record user feedback on memory usefulness.

        This feeds the evolution engine for continuous improvement.
        """
        try:
            # Create feedback memory for system learning
            feedback_content = f"FEEDBACK: Memory {memory_id} marked as {'useful' if useful else 'not useful'}"
            if notes:
                feedback_content += f" - Notes: {notes}"

            feedback_memory = MemoryRequest(
                content=feedback_content,
                metadata={
                    "type": "user_feedback",
                    "target_memory_id": memory_id,
                    "useful": useful,
                    "feedback_type": feedback_type,
                    "notes": notes,
                    "user_id": "system",
                    "conversation_id": "feedback_system",
                    "importance_score": 0.6  # Feedback is valuable for learning
                }
            )

            # Store feedback asynchronously
            asyncio.create_task(store.store_memory(feedback_memory))

            return {
                "message": "Feedback recorded successfully",
                "memory_id": memory_id,
                "useful": useful,
                "learning_impact": "This feedback will improve future memory scoring and evolution decisions"
            }

        except Exception as e:
            logger.error(f"Feedback recording failed: {e}")
            raise HTTPException(status_code=500, detail="Failed to record feedback")

    # =====================================================
    # DEDUPLICATION ENDPOINTS (Enterprise Features)
    # =====================================================

    @app.get("/dedup/stats")
    async def get_deduplication_stats(store: UnifiedVectorStore = Depends(get_store)):
        """
        Get comprehensive deduplication statistics.

        Shows duplicate detection rates, storage savings, and performance metrics.
        """
        try:
            if not store.deduplication_service:
                return {
                    "status": "disabled",
                    "message": "Deduplication service not enabled",
                    "enable_instructions": "Set DEDUPLICATION_MODE to 'log_only' or 'active'"
                }
            
            stats = await store.deduplication_service.get_stats()
            
            # Add unified store stats
            stats['store_stats'] = {
                'duplicates_prevented': store.stats.get('duplicates_prevented', 0),
                'storage_saved_bytes': store.stats.get('storage_saved_bytes', 0),
                'storage_saved_mb': round(store.stats.get('storage_saved_bytes', 0) / (1024 * 1024), 2)
            }
            
            return stats
            
        except Exception as e:
            logger.error(f"Failed to get deduplication stats: {e}")
            raise HTTPException(status_code=500, detail="Failed to get deduplication statistics")

    @app.post("/dedup/check")
    async def check_duplicate_content(
        content: str,
        metadata: dict[str, Any] | None = None,
        store: UnifiedVectorStore = Depends(get_store)
    ):
        """
        Check if content would be considered a duplicate.

        Useful for pre-checking before storing memories.
        """
        try:
            if not store.deduplication_service:
                raise HTTPException(status_code=503, detail="Deduplication service not enabled")
            
            result = await store.deduplication_service.check_duplicate(content, metadata)
            
            return {
                "is_duplicate": result.is_duplicate,
                "confidence_score": result.confidence_score,
                "decision": result.decision.value,
                "reason": result.reason,
                "existing_memory_id": str(result.existing_memory.id) if result.existing_memory else None,
                "content_hash": result.content_hash,
                "similarity_score": result.similarity_score
            }
            
        except Exception as e:
            logger.error(f"Duplicate check failed: {e}")
            raise HTTPException(status_code=500, detail="Failed to check for duplicates")

    @app.post("/dedup/review/{memory_id}")
    async def mark_false_positive(
        memory_id: str,
        actual_unique_id: str,
        store: UnifiedVectorStore = Depends(get_store)
    ):
        """
        Mark a deduplication decision as false positive.

        This helps improve the deduplication system over time.
        """
        try:
            if not store.deduplication_service:
                raise HTTPException(status_code=503, detail="Deduplication service not enabled")
            
            from uuid import UUID
            await store.deduplication_service.mark_false_positive(
                UUID(memory_id), 
                UUID(actual_unique_id)
            )
            
            return {
                "status": "success",
                "message": "False positive recorded successfully",
                "impact": "Future deduplication accuracy will be improved"
            }
            
        except ValueError:
            raise HTTPException(status_code=400, detail="Invalid UUID format")
        except Exception as e:
            logger.error(f"Failed to mark false positive: {e}")
            raise HTTPException(status_code=500, detail="Failed to record false positive")

    @app.delete("/dedup/cleanup")
    async def cleanup_orphaned_hashes(
        days: int = 90,
        admin_key: str | None = None,
        store: UnifiedVectorStore = Depends(get_store)
    ):
        """
        Clean up orphaned content hashes from deleted memories.

        Requires admin key for safety.
        """
        # Simple security check
        if admin_key != os.getenv("ADMIN_KEY", "dedup-cleanup-2025"):
            raise HTTPException(status_code=403, detail="Invalid admin key")
        
        try:
            if not store.deduplication_service:
                raise HTTPException(status_code=503, detail="Deduplication service not enabled")
            
            deleted_count = await store.deduplication_service.cleanup_old_hashes(days)
            
            return {
                "status": "success",
                "deleted_hashes": deleted_count,
                "message": f"Cleaned up {deleted_count} orphaned hashes older than {days} days"
            }
            
        except Exception as e:
            logger.error(f"Cleanup failed: {e}")
            raise HTTPException(status_code=500, detail="Failed to cleanup orphaned hashes")

    @app.post("/dedup/backfill")
    async def backfill_content_hashes(
        limit: int = 1000,
        admin_key: str | None = None,
        store: UnifiedVectorStore = Depends(get_store)
    ):
        """
        Backfill content hashes for existing memories.

        Useful when enabling deduplication on existing data.
        """
        # Simple security check
        if admin_key != os.getenv("ADMIN_KEY", "dedup-backfill-2025"):
            raise HTTPException(status_code=403, detail="Invalid admin key")
        
        try:
            # Get pgvector provider
            pgvector = store.providers.get('pgvector')
            if not pgvector or not pgvector.enabled:
                raise HTTPException(status_code=503, detail="PgVector provider not available")
            
            async with pgvector.connection_pool.acquire() as conn:
                # Find memories without hashes
                rows = await conn.fetch("""
                    SELECT vm.id, vm.content
                    FROM vector_memories vm
                    LEFT JOIN memory_content_hashes mch ON vm.id = mch.memory_id
                    WHERE mch.id IS NULL
                    ORDER BY vm.created_at DESC
                    LIMIT $1
                """, limit)
                
                # Hash and insert
                hashed_count = 0
                for row in rows:
                    try:
                        content_hash = hashlib.sha256(row['content'].encode()).hexdigest()
                        await conn.execute("""
                            INSERT INTO memory_content_hashes (content_hash, memory_id, content_length)
                            VALUES ($1, $2, $3)
                            ON CONFLICT (content_hash, memory_id) DO NOTHING
                        """, content_hash, row['id'], len(row['content']))
                        hashed_count += 1
                    except Exception as e:
                        logger.error(f"Failed to hash memory {row['id']}: {e}")
                
                return {
                    "status": "success",
                    "memories_processed": len(rows),
                    "hashes_created": hashed_count,
                    "message": f"Backfilled {hashed_count} content hashes"
                }
                
        except Exception as e:
            logger.error(f"Backfill failed: {e}")
            raise HTTPException(status_code=500, detail="Failed to backfill content hashes")

    # =====================================================
    # KNOWLEDGE GRAPH ENDPOINTS (Added by Agent 2)
    # =====================================================

    @app.post("/graph/sync/{memory_id}")
    async def sync_memory_to_graph(
        memory_id: str,
        store: UnifiedVectorStore = Depends(get_store)
    ):
        """
        Sync a specific memory to the knowledge graph.

        Extracts entities and relationships from an existing memory.
        """
        try:
            # Check if graph provider is available
            graph_provider = store.providers.get('graph')
            if not graph_provider or not graph_provider.enabled:
                raise HTTPException(status_code=503, detail="Graph provider not available")

            from uuid import UUID

            memory = await graph_provider.fetch_memory(UUID(memory_id))
            if not memory:
                raise HTTPException(status_code=404, detail="Memory not found")

            entities, relationships = await graph_provider.sync_existing_memory(memory)

            return {
                "memory_id": memory_id,
                "entities_added": entities,
                "relationships_added": relationships,
            }

        except Exception as e:
            logger.error(f"Graph sync failed: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to sync memory: {str(e)}")

    @app.get("/graph/explore/{entity_name}")
    async def explore_entity_relationships(
        entity_name: str,
        max_depth: int = 2,
        limit: int = 20,
        store: UnifiedVectorStore = Depends(get_store)
    ):
        """
        Explore relationships from a specific entity.

        Returns connected entities and their relationships up to max_depth.
        """
        try:
            # Validate inputs
            from .validators import validate_entity_name, validate_graph_depth
            entity_name = validate_entity_name(entity_name)
            max_depth = validate_graph_depth(max_depth)

            graph_provider = store.providers.get('graph')
            if not graph_provider or not graph_provider.enabled:
                raise HTTPException(status_code=503, detail="Graph provider not available")

            # Query memories filtered by entity
            filters = {"entity_name": entity_name}
            memories = await graph_provider.query([], limit, filters)

            return {
                "entity": entity_name,
                "max_depth": max_depth,
                "memories_found": len(memories),
                "memories": [
                    {
                        "id": str(mem.id),
                        "content": mem.content[:200] + "..." if len(mem.content) > 200 else mem.content,
                        "importance": mem.importance_score,
                        "similarity": mem.similarity_score
                    }
                    for mem in memories
                ]
            }

        except Exception as e:
            logger.error(f"Entity exploration failed: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to explore entity: {str(e)}")

    @app.get("/graph/path/{from_entity}/{to_entity}")
    async def find_entity_path(
        from_entity: str,
        to_entity: str,
        max_depth: int = 3,
        store: UnifiedVectorStore = Depends(get_store)
    ):
        """
        Find the shortest path between two entities in the knowledge graph.

        Uses graph traversal to find connections.
        """
        try:
            graph_provider = store.providers.get('graph')
            if not graph_provider or not graph_provider.enabled:
                raise HTTPException(status_code=503, detail="Graph provider not available")

            path = await graph_provider.find_path(from_entity, to_entity, max_depth)

            return {
                "from": from_entity,
                "to": to_entity,
                "path": path,
                "path_found": bool(path),
            }

        except Exception as e:
            logger.error(f"Path finding failed: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to find path: {str(e)}")

    @app.get("/graph/insights/{memory_id}")
    async def get_memory_graph_insights(
        memory_id: str,
        store: UnifiedVectorStore = Depends(get_store)
    ):
        """
        Get graph-based insights for a specific memory.

        Shows entities extracted and their relationships.
        """
        try:
            graph_provider = store.providers.get('graph')
            if not graph_provider or not graph_provider.enabled:
                raise HTTPException(status_code=503, detail="Graph provider not available")

            insights = await graph_provider.memory_insights(memory_id)
            if not insights:
                raise HTTPException(status_code=404, detail="Memory not found")

            return insights

        except Exception as e:
            logger.error(f"Graph insights failed: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to get insights: {str(e)}")

    @app.post("/graph/bulk-sync")
    async def bulk_sync_memories_to_graph(
        memory_ids: list[str],
        store: UnifiedVectorStore = Depends(get_store)
    ):
        """
        Sync multiple memories to the knowledge graph in bulk.

        Efficient batch processing for initial graph population.
        """
        try:
            graph_provider = store.providers.get('graph')
            if not graph_provider or not graph_provider.enabled:
                raise HTTPException(status_code=503, detail="Graph provider not available")

            from uuid import UUID
            ids = [UUID(mid) for mid in memory_ids]
            processed = await graph_provider.bulk_sync(ids)

            return {
                "status": "success",
                "memories_processed": processed,
            }

        except Exception as e:
            logger.error(f"Bulk sync failed: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to bulk sync: {str(e)}")

    @app.get("/graph/stats")
    async def get_graph_statistics(store: UnifiedVectorStore = Depends(get_store)):
        """
        Get comprehensive knowledge graph statistics.

        Shows entity counts, relationship types, and graph health.
        """
        try:
            graph_provider = store.providers.get('graph')
            if not graph_provider or not graph_provider.enabled:
                raise HTTPException(status_code=503, detail="Graph provider not available")

            stats = await graph_provider.get_stats()
            health = await graph_provider.health_check()

            return {
                "health": health,
                "statistics": stats
            }

        except Exception as e:
            logger.error(f"Graph stats failed: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to get graph stats: {str(e)}")

    @app.post("/admin/init-database")
    async def init_database_indexes(
        admin_key: str,
        store: UnifiedVectorStore = Depends(get_store)
    ):
        """
        Emergency endpoint to create missing database indexes.

        This fixes the query performance issue by creating the required pgvector indexes.
        """
        # Simple security check
        if admin_key != os.getenv("ADMIN_KEY", "emergency-fix-2024"):
            raise HTTPException(status_code=403, detail="Invalid admin key")

        pgvector_provider = None
        for name, provider in store.providers.items():
            if name == 'pgvector' and provider.enabled:
                pgvector_provider = provider
                break

        if not pgvector_provider:
            raise HTTPException(status_code=503, detail="pgvector provider not available")

        try:
            async with pgvector_provider.connection_pool.acquire() as conn:
                # Create the critical vector index
                await conn.execute("""
                    CREATE INDEX IF NOT EXISTS idx_vector_memories_embedding
                    ON vector_memories
                    USING ivfflat (embedding vector_cosine_ops)
                    WITH (lists = 100)
                """)

                # Create supporting indexes
                await conn.execute("""
                    CREATE INDEX IF NOT EXISTS idx_vector_memories_metadata
                    ON vector_memories USING GIN (metadata)
                """)

                await conn.execute("""
                    CREATE INDEX IF NOT EXISTS idx_vector_memories_importance
                    ON vector_memories (importance_score DESC)
                """)

                # Update statistics
                await conn.execute("ANALYZE vector_memories")

                # Verify indexes were created
                indexes = await conn.fetch("""
                    SELECT indexname
                    FROM pg_indexes
                    WHERE tablename = 'vector_memories'
                """)

                # Test query performance
                test_result = await pgvector_provider.query(
                    embedding=[0.1] * 1536,  # Mock embedding
                    limit=5
                )

                return {
                    "success": True,
                    "indexes_created": [idx['indexname'] for idx in indexes],
                    "test_query_returned": len(test_result),
                    "message": "Database indexes created successfully! Queries should now work."
                }

        except Exception as e:
            logger.error(f"Database initialization failed: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to initialize database: {str(e)}")

    @app.post("/graph/query")
    async def query_knowledge_graph(
        query: dict,
        store: UnifiedVectorStore = Depends(get_store)
    ):
        """
        Advanced graph query endpoint.

        Supports entity filtering, relationship traversal, and pattern matching.
        """
        try:
            graph_provider = store.providers.get('graph')
            if not graph_provider or not graph_provider.enabled:
                raise HTTPException(status_code=503, detail="Graph provider not available")

            # Convert query dict to filters for the provider
            filters = {}
            if query.get('entity_name'):
                filters['entity_name'] = query['entity_name']
            if query.get('entity_type'):
                filters['entity_type'] = query['entity_type']
            if query.get('relationship_type'):
                filters['relationship_type'] = query['relationship_type']

            # Execute query
            import time
            start_time = time.time()

            limit = query.get('limit', 10)
            await graph_provider.query([], limit, filters)

            query_time = (time.time() - start_time) * 1000

            # TODO: Convert memories to graph nodes and relationships
            return {
                "nodes": [],
                "relationships": [],
                "query_time_ms": query_time,
                "total_nodes": 0,
                "total_relationships": 0
            }

        except Exception as e:
            logger.error(f"Graph query failed: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to query graph: {str(e)}")

    # Knowledge Graph Live Sync Endpoints for Agent 3
    @app.get("/api/knowledge-graph/live-stats")
    async def knowledge_graph_live_stats(store: UnifiedVectorStore = Depends(get_store)):
        """Real-time stats for Agent 3 dashboard - poll every 10 seconds"""
        try:
            # Get pgvector provider's connection pool
            pgvector_provider = None
            for name, provider in store.providers.items():
                if name == 'pgvector' and provider.enabled:
                    pgvector_provider = provider
                    break

            if not pgvector_provider:
                raise HTTPException(status_code=503, detail="pgvector provider not available")

            async with pgvector_provider.connection_pool.acquire() as conn:
                # Get unique entity count
                entity_count = await conn.fetchval(
                    "SELECT COUNT(*) FROM graph_nodes"
                )

                # Get relationship count
                rel_count = await conn.fetchval(
                    "SELECT COUNT(*) FROM graph_relationships"
                )

                # Get top entities by connections
                top_entities = await conn.fetch("""
                    SELECT n.entity_name, n.entity_type, n.importance_score,
                           COUNT(DISTINCT r.to_node_id) + COUNT(DISTINCT r2.from_node_id) as connections
                    FROM graph_nodes n
                    LEFT JOIN graph_relationships r ON n.id = r.from_node_id
                    LEFT JOIN graph_relationships r2 ON n.id = r2.to_node_id
                    GROUP BY n.id, n.entity_name, n.entity_type, n.importance_score
                    ORDER BY connections DESC, n.importance_score DESC
                    LIMIT 10
                """)

                # Get entity type distribution
                type_dist = await conn.fetch("""
                    SELECT entity_type, COUNT(*) as count
                    FROM graph_nodes
                    GROUP BY entity_type
                    ORDER BY count DESC
                """)

                return JSONResponse({
                    "entity_count": entity_count,
                    "relationship_count": rel_count,
                    "top_entities": [
                        {
                            "name": e["entity_name"],
                            "type": e["entity_type"],
                            "importance": float(e["importance_score"]),
                            "connections": e["connections"]
                        }
                        for e in top_entities
                    ],
                    "entity_types": {
                        row["entity_type"]: row["count"]
                        for row in type_dist
                    },
                    "last_updated": datetime.utcnow().isoformat(),
                    "sync_version": "2.0",
                    "status": "live",
                    "extraction_complete": True,
                    "trust_crisis_resolved": True
                })
        except Exception as e:
            logger.error(f"Error getting live stats: {e}")
            return JSONResponse({"error": str(e), "status": "error"}, status_code=500)

    @app.post("/api/knowledge-graph/refresh-cache")
    async def refresh_dashboard_cache():
        """Signal Agent 3 to refresh its cache"""
        return JSONResponse({
            "cache_refresh_requested": True,
            "timestamp": datetime.utcnow().isoformat(),
            "message": "Agent 3 should refresh dashboard within 10 seconds"
        })

    @app.get("/api/knowledge-graph/sync-status")
    async def get_sync_status(store: UnifiedVectorStore = Depends(get_store)):
        """Check if Agent 2 and Agent 3 are in sync"""
        try:
            # Get current stats
            stats_response = await knowledge_graph_live_stats(store)
            stats = json.loads(stats_response.body)

            return JSONResponse({
                "agent2_stats": stats,
                "sync_instructions": {
                    "polling_interval": "10s",
                    "endpoint": "/api/knowledge-graph/live-stats",
                    "cache_key": "graph_stats_v2"
                },
                "deduplication_needed": stats.get("entity_count", 0) > 70
            })
        except Exception as e:
            logger.error(f"Sync status error: {e}")
            return JSONResponse({"error": str(e)}, status_code=500)

    return app


# For running with uvicorn
app = create_memory_app()

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "memory_service.api:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
